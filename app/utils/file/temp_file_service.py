import gc
import hashlib
import time
from contextlib import contextmanager
from datetime import datetime, timedelta
from pathlib import Path

import psutil
from celery.exceptions import Reject
from redis.exceptions import LockNotOwnedError, LockError

from app.config import FileConfig
from app.core.exception import ValidationError, logger, FileUploadError, RedisConnectionError, FileSaveError, \
    NotFoundError, FileCleanupError
from app.core.redis_connection_pool import redis_pool
from app.docker.core.celery_app import CeleryManager
from app.exts import db
from app.model.model_service import ModelService
from app.user.user_service import UserService
from app.utils.image_url_utils import ImageURLHandlerUtils
from app.utils.storage import storage, FileStorage


def remove_empty_parents_safely(file_path: Path, stop_at: Path, max_retries: int = 3):
    """
    å®‰å…¨åˆ é™¤ç©ºç›®å½•é“¾ï¼ˆå¸¦å¹¶å‘æ£€æŸ¥å’Œé‡è¯•æœºåˆ¶ï¼‰

    å‚æ•°:
        file_path: è¢«åˆ é™¤æ–‡ä»¶çš„è·¯å¾„
        stop_at: åœæ­¢åˆ é™¤çš„çˆ¶ç›®å½•ï¼ˆåŒ…å«è¯¥ç›®å½•æœ¬èº«ï¼‰
        max_retries: ç›®å½•æ“ä½œæœ€å¤§é‡è¯•æ¬¡æ•°
    """
    # if not file_path.is_relative_to(Path(FileConfig.TEMP_DIR).resolve()):
    #     logger.warning(f"âš ï¸ æ‹’ç»å¤„ç†éä¸´æ—¶ç›®å½•: {file_path}")
    #     return

    logger.info("file_path: %s, stop_at: %s", file_path, stop_at)

    current_dir = file_path.parent

    while current_dir != stop_at and current_dir.is_relative_to(stop_at):
        logger.debug("â–¶ å¼€å§‹å¤„ç†ç›®å½•: %s", current_dir)

        # é‡è¯•æœºåˆ¶åº”å¯¹çŸ­æš‚çš„æ–‡ä»¶ç³»ç»Ÿå»¶è¿Ÿ
        for attempt in range(max_retries):
            try:
                if not any(current_dir.iterdir()):
                    logger.debug(f"â†˜ å°è¯•åˆ é™¤ç©ºç›®å½•ï¼ˆç¬¬{attempt + 1}æ¬¡å°è¯•ï¼‰: {current_dir}")
                    current_dir.rmdir()
                    logger.info(f"ğŸ—‘ï¸ æˆåŠŸåˆ é™¤ç©ºç›®å½•: {current_dir}")
                    break
                else:
                    logger.warning(f"âš ï¸ ç›®å½•éç©ºï¼Œåœæ­¢å‘ä¸Šåˆ é™¤: {current_dir}")
                    return
            except FileNotFoundError:
                logger.debug(f"â†— ç›®å½•å·²è¢«å…¶ä»–è¿›ç¨‹åˆ é™¤: {current_dir}")
                break
            except PermissionError as e:
                logger.error(f"â›” ç›®å½•æƒé™é”™è¯¯: {current_dir} - {str(e)}")
                return
            except OSError as e:
                if attempt == max_retries - 1:
                    logger.warning(f"âš ï¸ åˆ é™¤ç›®å½•å¤±è´¥ï¼ˆå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°ï¼‰: {current_dir} - {str(e)}")
                    return
                logger.debug(f"â†» é‡åˆ°æš‚æ—¶æ€§é”™è¯¯ï¼Œå‡†å¤‡é‡è¯•: {current_dir} - {str(e)}")
                time.sleep(0.5 * (attempt + 1))
        else:
            return

        # å‘ä¸Šç§»åŠ¨ä¸€çº§ç›®å½•ï¼ˆä½¿ç”¨è§£æåçš„ç»å¯¹è·¯å¾„é¿å…ç¬¦å·é“¾æ¥é—®é¢˜ï¼‰
        parent_dir = current_dir.resolve().parent
        if parent_dir == current_dir.resolve():
            logger.debug("â¹ åˆ°è¾¾æ ¹ç›®å½•ï¼Œåœæ­¢å¤„ç†")
            break
        current_dir = parent_dir


class TempFileService:
    def __init__(self):
        self.storage = storage
        self.redis = redis_pool

    @staticmethod
    @contextmanager
    def _get_redis_conn():
        """å¤ç”¨Redisè¿æ¥ç®¡ç†"""
        with redis_pool.get_redis_connection('files') as conn:
            yield conn

    @staticmethod
    def cleanup_temp_files(src_file: Path, is_temp: bool = True):
        """åŸå­åŒ–æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        try:
            if src_file.exists():
                src_file.unlink(missing_ok=True)
                # æ ¹æ®æ–‡ä»¶ç±»å‹è®¾ç½® stop_at
                stop_at = (
                    Path(FileConfig.TEMP_DIR).resolve()
                    if is_temp
                    else Path(FileConfig.FORMAL_RIR).resolve()
                )
                remove_empty_parents_safely(
                    file_path=src_file,
                    stop_at=stop_at,
                    max_retries=3
                )
        except Exception as e:
            logger.error(f"æ–‡ä»¶æ¸…ç†å¤±è´¥: {e}")
            raise FileCleanupError("æ–‡ä»¶æ¸…ç†å¤±è´¥")  # è‡ªå®šä¹‰å¼‚å¸¸

    @staticmethod
    def _update_redis_status(redis_key: str, error: Exception = None):
        """æ›´æ–°RedisçŠ¶æ€"""
        try:
            with redis_pool.get_redis_connection('files') as conn:
                if isinstance(error, FileNotFoundError):
                    conn.delete(redis_key)
                elif error:
                    conn.hset(redis_key, "status", "error")
                else:
                    conn.delete(redis_key)
        except Exception as e:
            logger.error(f"RedisçŠ¶æ€æ›´æ–°å¤±è´¥: {e}")

    def save_temp(self, file, upload_type: str, data_id: int, file_type: str, user_id: int) -> str:
        """ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•å¹¶ç”ŸæˆURL"""
        try:
            # ç”Ÿæˆæ–‡ä»¶å“ˆå¸Œï¼ˆä½¿ç”¨æ–‡ä»¶å†…å®¹ï¼‰
            logger.info(f"upload_type: {upload_type}, file_type: {file_type}, data_id: {data_id}, file: {file}")

            # ========== æ–‡ä»¶å†…å®¹å¤„ç† ==========
            try:
                file_content = file.read()
                file_hash = hashlib.md5(file_content).hexdigest()
                file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
            except IOError as e:
                logger.error("æ–‡ä»¶è¯»å–å¤±è´¥: %s", e)
                raise FileUploadError("æ–‡ä»¶æŸåæˆ–æ— æ³•è¯»å–")

            logger.info(f"âœ… è®¡ç®—çš„æ–‡ä»¶å“ˆå¸Œ: {file_hash}")
            logger.info(f"âœ… ä¿å­˜çš„æ–‡ä»¶å: {file_hash}{Path(file.filename).suffix}")

            # ========== Redisé‡å¤æ£€æŸ¥ ==========
            version = datetime.now().strftime("%Y%m%d%H%M%S")
            redis_key = f"temp:{user_id}:{upload_type}:{data_id}:{file_type}:{version}:{file_hash}"
            try:
                with self.redis.get_redis_connection('files') as conn:
                    if conn.exists(redis_key):
                        current_status = conn.hget(redis_key, "status")
                        if current_status:
                            if current_status in ['pending', 'processing']:
                                raise FileUploadError("æ–‡ä»¶å·²ä¸Šä¼ ï¼Œè¯·å‹¿é‡å¤æäº¤")
            except RedisConnectionError as e:
                logger.error("Redisè¿æ¥å¼‚å¸¸: %s", e)
                raise FileUploadError("ç³»ç»Ÿç¹å¿™ï¼Œè¯·ç¨åå†è¯•")

            # ========== ä¸´æ—¶ç›®å½•æ„å»º ==========
            try:
                temp_dir = Path(FileConfig.TEMP_DIR) / FileConfig.UPLOAD_CONFIG[upload_type]['subdirectory'].format(
                    file_type=file_type,
                    data_id=data_id,
                    user_id=user_id,
                    version=version
                )
            except KeyError:
                logger.error("æ— æ•ˆçš„ä¸Šä¼ ç±»å‹: %s", upload_type)
                raise FileUploadError("ä¸æ”¯æŒçš„ä¸Šä¼ ç±»å‹")

            # ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•
            # ========== æ–‡ä»¶å­˜å‚¨ ==========
            try:
                saved_path = self.storage.save_upload(
                    file_stream=file,
                    save_dir=temp_dir,
                    file_name=f"{file_hash}{Path(file.filename).suffix}"
                )
                saved_file_path = Path(saved_path) / f"{file_hash}{Path(file.filename).suffix}"
            except FileSaveError as e:
                logger.error("æ–‡ä»¶å­˜å‚¨å¤±è´¥: %s", e)
                raise FileUploadError("æ–‡ä»¶ä¿å­˜å¤±è´¥")
            logger.info(f"ä¸´æ—¶ä¿å­˜ç›®å½•: {saved_path}")
            logger.info(f"âœ… å®Œæ•´æ–‡ä»¶è·¯å¾„: {saved_file_path}")

            # ========== Redisè®°å½• ==========
            try:
                with self.redis.get_redis_connection('files') as conn:
                    conn.hmset(redis_key, {
                        "real_path": str(saved_file_path),
                        "user_id": user_id,
                        "status": "pending",
                        "expire_at": int(time.time() + 10)
                    })
                    # conn.expire(key, 120)  # åŸä¸º7å¤©ï¼ˆ604800ç§’ï¼‰
            except Exception as e:
                logger.error("Redisè®°å½•å¤±è´¥: %s", e)
                # å›æ»šå·²ä¿å­˜æ–‡ä»¶
                try:
                    saved_file_path.unlink(missing_ok=True)
                    remove_empty_parents_safely(saved_file_path, Path(FileConfig.TEMP_DIR))
                except Exception as cleanup_err:
                    logger.error("æ–‡ä»¶å›æ»šå¤±è´¥: %s", cleanup_err)
                raise FileUploadError("ç³»ç»Ÿä¸´æ—¶é”™è¯¯")

            logger.info(f"âœ… ä¿å­˜åˆ°Redisçš„é”®: {redis_key}")
            logger.info(f"âœ… æ–‡ä»¶å“ˆå¸Œ: {file_hash}")
            logger.info(f"âœ… ä¿å­˜è·¯å¾„: {saved_path}")

            # è·å–ç›¸å¯¹äº TEMP_DIR çš„è·¯å¾„
            relative_path = Path(saved_path).relative_to(FileConfig.TEMP_DIR)

            # ç”ŸæˆURLè·¯å¾„ï¼ˆä½¿ç”¨å®é™…å­˜å‚¨è·¯å¾„ç»“æ„ï¼‰
            url_path = f"{FileConfig.TEMP_BASE_URL}/{relative_path}/{file_hash}{Path(file.filename).suffix}"
            logger.info(f"âœ… ç”Ÿæˆçš„URLç›¸å¯¹è·¯å¾„: {url_path}")

            return url_path
        except FileUploadError:
            raise
        except Exception as e:
            logger.error("æœªçŸ¥å¼‚å¸¸", exc_info=True)
            raise FileUploadError("ä¸Šä¼ æœåŠ¡å¼‚å¸¸")  # å…œåº•å¼‚å¸¸å¤„ç†

    def commit_from_temp(self, temp_url: str, user_id: int) -> bool:
        """ä»ä¸´æ—¶URLæäº¤æ–‡ä»¶ï¼ˆå…¶ä»–æ¥å£è°ƒç”¨ï¼‰"""
        # è§£æURLå‚æ•°
        try:
            # ä½¿ç”¨å·¥å…·ç±»éªŒè¯å’Œè§£æURL
            ImageURLHandlerUtils.validate_photo_file(temp_url)
            url_components = ImageURLHandlerUtils.parse_temp_url_components(temp_url)
            redis_key = ImageURLHandlerUtils.build_temp_redis_key(url_components)

            # è®°å½•è°ƒè¯•æ—¥å¿—
            logger.debug(f"è§£æç»“æœ: {url_components}")

            # RediséªŒè¯
            with self.redis.get_redis_connection('files') as conn:
                file_info = conn.hgetall(redis_key)
                logger.debug(f"Redisè¿”å›æ•°æ®: {file_info} | ç±»å‹: {type(file_info)}")

                if not isinstance(file_info, dict):
                    logger.error(f"Redisæ•°æ®æ ¼å¼é”™è¯¯ï¼ŒæœŸæœ›å­—å…¸ï¼Œå®é™…å¾—åˆ°: {type(file_info)}")
                    raise ValidationError("æ–‡ä»¶å…ƒæ•°æ®æŸå")

                if not file_info:
                    logger.error(f"Redisé”®ä¸å­˜åœ¨: {redis_key}")
                    raise ValidationError("æ–‡ä»¶ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ")

                if 'user_id' not in file_info:
                    logger.error(f"Redisè®°å½•ç¼ºå°‘user_idå­—æ®µ: {file_info}")
                    raise ValidationError("æ–‡ä»¶å…ƒæ•°æ®ä¸å®Œæ•´")

                if file_info.get('user_id') != str(user_id):
                    logger.warning(
                        f"ç”¨æˆ·æƒé™éªŒè¯å¤±è´¥: Redis.user_id={file_info.get('user_id')} vs Current.user_id={user_id}")
                    raise ValidationError("æ— æ“ä½œæƒé™")

                if file_info.get('status') == 'committed':
                    logger.warning(f"æ–‡ä»¶é‡å¤æäº¤: {redis_key}")
                    raise ValidationError("æ–‡ä»¶å·²æäº¤")

                # æ›´æ–°çŠ¶æ€
                conn.hset(redis_key, "status", "processing")
                logger.info(f"çŠ¶æ€æ›´æ–°ä¸ºprocessing: {redis_key}")

            # å¼‚æ­¥è½¬ç§»æ–‡ä»¶
            self._move_to_final.delay(
                redis_key=redis_key,
                src_path=file_info['real_path'],
                upload_type=url_components['upload_type'],
                data_id=url_components['data_id'],
                file_type=url_components['file_type'],
                user_id=user_id,
                version=url_components['version']
            )
            logger.info(f"âœ… è§£æå¾—åˆ°çš„å‚æ•°: {url_components}")
            logger.info(f"âœ… ç”Ÿæˆçš„Redisé”®: {redis_key}")

            return True
        except ValidationError as e:
            logger.error("æ–‡ä»¶éªŒè¯å¤±è´¥ï¼š%s", str(e))
            raise e
        except Exception as e:
            logger.error("æäº¤æ–‡ä»¶æ—¶å‘ç”Ÿæœªæ•è·çš„å¼‚å¸¸: %s", str(e))
            raise e

    @staticmethod
    @CeleryManager.get_celery().task(bind=True, expires=86400)
    def _move_to_final(self, redis_key: str, src_path: str, upload_type: str, data_id: int, file_type: str,
                       user_id: int, version: str) -> None:
        """åŸå­åŒ–æ–‡ä»¶è½¬ç§»æ“ä½œï¼ˆå¢å¼ºå¥å£®æ€§ï¼‰"""
        src_file = Path(src_path)
        final_file_path = None
        error = None

        try:
            src_file = Path(src_path)
            # ========== 1. åˆå§‹åŒ–æ£€æŸ¥ ==========
            from myapp import create_app
            app = create_app()  # åˆ›å»ºFlaskä¸Šä¸‹æ–‡

            # ========== 2. å‰ç½®çŠ¶æ€éªŒè¯ ==========
            with redis_pool.get_redis_connection('files') as conn:
                # æ£€æŸ¥Redisé”®æ˜¯å¦å­˜åœ¨ï¼Œé˜²æ­¢é‡å¤å¤„ç†
                if not conn.exists(redis_key):
                    logger.warning(f"â© è·³è¿‡å·²å¤„ç†çš„ä»»åŠ¡: {redis_key}")
                    return

                # è·å–æ–‡ä»¶ä¿¡æ¯
                file_info = conn.hgetall(redis_key)
                if file_info.get('status') == 'committed':
                    logger.warning(f"â© æ–‡ä»¶å·²æäº¤: {redis_key}")
                    return

                # æ ‡è®°ä¸ºå¤„ç†ä¸­ï¼ˆé˜²æ­¢å¹¶å‘ï¼‰
                conn.hset(redis_key, "status", "processing")

            # ========== 3. æ–‡ä»¶æ“ä½œ ==========
            if not src_file.exists():
                logger.error(f"â›” æºæ–‡ä»¶ä¸å­˜åœ¨: {src_path}")
                with redis_pool.get_redis_connection('files') as conn:
                    conn.delete(redis_key)  # æ¸…ç†æ— æ•ˆé”®
                return

            # 3.1 ç§»åŠ¨æ–‡ä»¶åˆ°æ­£å¼ç›®å½•
            # æ„å»ºæ­£å¼ç›®å½•
            final_subdir = FileConfig.UPLOAD_CONFIG[upload_type]['subdirectory'].format(
                file_type=file_type,
                data_id=data_id,
                user_id=user_id,
                version=version
            )
            final_dir = Path(FileConfig.LOCAL_FILE_BASE) / "user_data" / final_subdir
            final_dir.mkdir(parents=True, exist_ok=True)
            final_file_path = final_dir / src_file.name

            # ä½¿ç”¨æ–‡ä»¶æµé¿å…å ç”¨æ–‡ä»¶é”
            with open(src_path, 'rb') as src_stream:
                FileStorage.save_upload(
                    file_stream=src_stream,
                    save_dir=final_dir,
                    file_name=src_file.name
                )

            # ========== 4. æ•°æ®åº“æ›´æ–° ==========
            relative_path = str(final_file_path.relative_to(FileConfig.LOCAL_FILE_BASE))
            logger.info("æ–°æ–‡ä»¶å­˜æ”¾ç›¸å¯¹è·¯å¾„: %s", relative_path)

            database_mapping = {
                "model": {
                    "icon": (ModelService.get_model_by_id, "icon")  # (æ¨¡å‹æŸ¥è¯¢æ–¹æ³•, å­—æ®µå)
                },
                "user": {
                    "avatars": (UserService.get_user_by_id, "avatar")
                }
            }

            # åœ¨åº”ç”¨ä¸Šä¸‹æ–‡ä¸­æ›´æ–°æ•°æ®åº“
            with app.app_context():
                db_committed = False
                try:
                    # æ›´æ–°æ•°æ®åº“
                    if upload_type in database_mapping and file_type in database_mapping[upload_type]:
                        get_func, field = database_mapping[upload_type][file_type]
                        instance = get_func(data_id)

                        old_relative_path = getattr(instance, field)
                        logger.info("æ—§æ–‡ä»¶å­˜æ”¾ç›¸å¯¹è·¯å¾„ï¼š%s", old_relative_path)

                        # æ›´æ–°æ•°æ®åº“
                        setattr(instance, field, relative_path)
                        db.session.commit()
                        db_committed = True

                        # è·å–æ—§è·¯å¾„å¹¶åˆ é™¤å¯¹åº”çš„æ–‡ä»¶
                        if file_type in FileConfig.SINGLE_FILE_TYPES:
                            if old_relative_path:
                                old_file = Path(FileConfig.LOCAL_FILE_BASE) / old_relative_path
                                if old_file.exists():
                                    try:
                                        TempFileService.cleanup_temp_files(old_file, is_temp=False)
                                        logger.info("ğŸ—‘ï¸ åˆ é™¤æ—§æ–‡ä»¶: %s", old_file)
                                    except Exception as e:
                                        logger.error("â›” æ—§æ–‡ä»¶åˆ é™¤å¤±è´¥: %s", str(e))

                except Exception as e:
                    error = e
                    db.session.rollback()
                    logger.error("â›” æ•°æ®åº“æ›´æ–°å¤±è´¥: %s", str(e))

            TempFileService._update_redis_status(redis_key)

        except FileNotFoundError as e:
            error = e
            # 6. æ–‡ä»¶ä¸å­˜åœ¨æ—¶çš„ä¸“ç”¨å¤„ç†
            logger.error(f"ğŸ›‘ æ–‡ä»¶å·²åˆ é™¤ï¼Œç»ˆæ­¢ä»»åŠ¡: {src_path}")
            with redis_pool.get_redis_connection('files') as conn:
                conn.delete(redis_key)  # ç¡®ä¿æ¸…ç†æ®‹ç•™
            return  # ç›´æ¥è¿”å›ï¼Œä¸é‡è¯•
        except Exception as e:
            error = e
            # 7. å…¶ä»–é”™è¯¯å¤„ç†
            logger.error(f"è¿ç§»å¤±è´¥: {str(e)}")
            with redis_pool.get_redis_connection('files') as conn:
                conn.hset(redis_key, "status", "error")
            raise self.retry(exc=e, countdown=60, max_retries=2)  # æœ‰é™é‡è¯•
        finally:
            # ========== ç¡®ä¿æœ€ç»ˆæ¸…ç† ==========
            # æ— è®ºæˆåŠŸä¸å¦ï¼Œéƒ½å°è¯•æ¸…ç†ä¸´æ—¶æ–‡ä»¶åŠç›®å½•
            TempFileService.cleanup_temp_files(src_file, is_temp=True)

            # å¦‚æœæ–°æ–‡ä»¶å·²åˆ›å»ºå¹¶æœªæäº¤åˆ°æ•°æ®åº“ä¸­ä¸”ä»»åŠ¡å¤±è´¥ï¼Œæ¸…ç†æ–°æ–‡ä»¶
            if error and final_file_path and final_file_path.exists() and not db_committed:
                try:
                    logger.info("å›æ»šåˆ é™¤æ–°æ–‡ä»¶: %s", final_file_path)
                    TempFileService.cleanup_temp_files(final_file_path, is_temp=False)
                except Exception as e:
                    logger.error("æ–°æ–‡ä»¶æ¸…ç†å¤±è´¥: %s", str(e))


temp_service = TempFileService()


#
# @CeleryManager.get_celery().task(bind=True, expires=86400)
# def cleanup_temp_files(self):
#     logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œä¸´æ—¶æ–‡ä»¶æ¸…ç†ä»»åŠ¡")
#     try:
#         with redis_pool.get_redis_connection('files') as conn:
#             cursor = '0'
#             deleted_count = 0
#             total_scanned = 0  # ç»Ÿè®¡æ‰«æçš„é”®æ•°é‡
#             while True:
#                 cursor, keys = conn.scan(cursor, match='temp:*', count=500)
#                 logger.info(f"ğŸ” æ‰«æåˆ° {len(keys)} ä¸ªé”®ï¼Œcursor={cursor}")  # æ–°å¢æ—¥å¿—
#                 total_scanned += len(keys)
#                 if not keys and cursor == 0:  # é€€å‡ºæ¡ä»¶ï¼šæ— é”®ä¸”æ¸¸æ ‡å½’é›¶
#                     break
#
#                 for key in keys:
#                     try:
#                         # ç¡®ä¿æ¯æ¬¡å¾ªç¯éƒ½åˆå§‹åŒ–å˜é‡
#                         file_path_str = None
#                         file_path_obj = None
#
#                         fields = conn.hmget(key, ['expire_at', 'real_path'])
#                         expire_at, file_path_str = fields[0], fields[1]
#                         logger.debug(f"æ£€æŸ¥é”®: {key}, expire_at={expire_at}")
#                         # æ¡ä»¶åˆ¤æ–­å’Œè·¯å¾„è½¬æ¢
#                         if expire_at and float(expire_at) < time.time():
#                             if file_path_str:
#                                 file_path_obj = Path(file_path_str)
#                                 logger.debug(f"å¤„ç†è¿‡æœŸé”®: {key}, æ–‡ä»¶è·¯å¾„={file_path_obj}")
#
#                                 # åˆ é™¤æ–‡ä»¶
#                                 if file_path_obj.exists() and file_path_obj.is_file():
#                                     file_path_obj.unlink()
#                                     logger.info(f"ğŸ—‘ï¸ åˆ é™¤æ–‡ä»¶: {file_path_obj}")
#
#                                     # æ¸…ç†ç©ºç›®å½•ï¼ˆæ¯æ¬¡åˆ é™¤æ–‡ä»¶åç«‹å³æ‰§è¡Œï¼‰
#                                     remove_empty_parents_safely(
#                                         file_path=file_path_obj,
#                                         stop_at=Path(FileConfig.TEMP_DIR).resolve(),
#                                         max_retries=2
#                                     )
#                                 else:
#                                     logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨æˆ–éæ–‡ä»¶: {file_path_obj}")
#
#                             # åˆ é™¤ Redis é”®
#                             conn.delete(key)
#                             deleted_count += 1
#                             logger.info(f"âœ… æ¸…ç†å®Œæˆ: {key}")
#
#                     except Exception as e:
#                         logger.error(f"æ¸…ç†å¤±è´¥: {key} - {str(e)}", exc_info=True)
#
#                 if cursor == 0:  # æ¸¸æ ‡å½’é›¶æ—¶é€€å‡º
#                     break
#
#             logger.info(f"ğŸ‰ æ¸…ç†å®Œæˆï¼Œå…±æ‰«æ {total_scanned} ä¸ªé”®ï¼Œåˆ é™¤ {deleted_count} ä¸ªè¿‡æœŸæ–‡ä»¶")
#
#     except Exception as e:
#         logger.error("æ¸…ç†ä»»åŠ¡å‘ç”Ÿå…¨å±€é”™è¯¯: %s", str(e), exc_info=True)
#         raise self.retry(exc=e, countdown=300, max_retries=3)

@CeleryManager.get_celery().task(
    bind=True,
    autoretry_for=(Exception,),
    max_retries=3,
    retry_backoff=30,
    time_limit=300,  # 5åˆ†é’Ÿè¶…æ—¶
    soft_time_limit=280
)
def cleanup_temp_files(self):
    lock_key = "cleanup_temp_files_lock"
    max_memory = 512 * 1024 * 1024  # 512MBæ›´å®‰å…¨

    try:
        with redis_pool.get_redis_connection('files') as conn:
            # ä½¿ç”¨ç»Ÿä¸€è¿æ¥åˆ›å»ºé”å¯¹è±¡
            lock = conn.lock(
                name=lock_key,
                timeout=300,  # ä¸ä»»åŠ¡è¶…æ—¶å¯¹é½
                blocking_timeout=15,
                thread_local=False
            )

            # åŸå­åŒ–é”æ“ä½œ
            if not lock.acquire(blocking=False):
                current_owner = conn.get(f"{lock_key}:holder")
                logger.warning(f"â³ æ¸…ç†ä»»åŠ¡è¿›è¡Œä¸­ | æŒæœ‰è€…: {safe_redis_decode(current_owner)}")
                raise Reject("å·²æœ‰è¿è¡Œä¸­çš„æ¸…ç†ä»»åŠ¡", requeue=False)

            try:
                # è®°å½•é”æŒæœ‰è€…
                conn.setex(f"{lock_key}:holder", 300, f"Task:{self.request.id}")
                logger.info(f"ğŸ”’ é”è·å–æˆåŠŸ | ä»»åŠ¡ID: {self.request.id}")
                logger.info("â˜… è¿›å…¥æ ¸å¿ƒæ¸…ç†æµç¨‹")

                # æ ¸å¿ƒæ¸…ç†é€»è¾‘(ä¿æŒåŸå¤„ç†æµç¨‹)
                result = execute_cleanup_process(conn, max_memory)
                logger.info(f"â˜… æ¸…ç†å®Œæˆï¼Œå…±åˆ é™¤ {result} ä¸ªRedisé”®")  # æ–°å¢
                return result

            finally:
                try:
                    lock.release()
                    conn.delete(f"{lock_key}:holder")
                    logger.info("ğŸ”“ é”å·²é‡Šæ”¾")
                except LockNotOwnedError:
                    logger.warning("âš ï¸ é”å·²è‡ªåŠ¨è¿‡æœŸ")

    except (LockError, ConnectionError) as e:
        logger.error("ğŸ”‘ é”æ“ä½œå¼‚å¸¸: %s", str(e))
        raise self.retry(exc=e, countdown=60)
    finally:
        gc.collect()


def execute_cleanup_process(conn, max_memory):
    """å®Œæ•´çš„æ¸…ç†æµç¨‹å®ç°ï¼ˆå¸¦å†…å­˜/è¶…æ—¶ä¿æŠ¤ï¼‰"""
    deleted_count = 0  # å®é™…å·²åˆ é™¤çš„Redisé”®è®¡æ•°
    expired_data_buffer = []  # # å­˜å‚¨å¾…åˆ é™¤çš„ (key, path) å…ƒç»„
    batch_size = 200
    start_time = time.time()
    last_log_time = start_time

    try:
        # ====== è¯Šæ–­æ—¥å¿— ======
        logger.info(f"â˜… å½“å‰Redisé…ç½®: {conn.connection_pool.connection_kwargs}")

        # ====== 1. åˆå§‹åŒ–æ‰«æ ======
        cursor = 0
        total_scanned = 0
        logger.info("â˜… å¼€å§‹æ‰«æRedisä¸´æ—¶é”®")

        # å†…å­˜ä¿æŠ¤æ£€æŸ¥ç‚¹
        def memory_guard():
            current_mem = psutil.Process().memory_info().rss
            if current_mem > max_memory:
                logger.error(f"ğŸ›‘ å†…å­˜è¶…é™: {current_mem // 1024 // 1024}MB > {max_memory // 1024 // 1024}MB")
                raise MemoryError("å†…å­˜è¶…é™ä¿æŠ¤è§¦å‘")

        # ====== 2. ä¸»æ‰«æå¾ªç¯ ======
        logger.info("cursorä¸ºï¼š%s", cursor)
        while True:
            # æ£€æŸ¥å†…å­˜çŠ¶æ€ï¼ˆæ¯æ¬¡å¾ªç¯å‰ï¼‰
            memory_guard()

            # å‘¨æœŸæ€§æ—¥å¿—ï¼ˆæ¯30sï¼‰
            if time.time() - last_log_time > 30:
                logger.info(
                    f"â±ï¸ å·²è¿è¡Œ: {int(time.time() - start_time)}ç§’ | "
                    f"æ‰«æé”®: {total_scanned} | æ¸…ç†æ–‡ä»¶: {deleted_count}"
                )
                last_log_time = time.time()

            # ====== 2.1 åˆ†æ‰¹æ¬¡SCAN ======
            next_cursor, keys = conn.scan(
                cursor=cursor,
                match='temp:*',  # é™å®šé”®åå‰ç¼€
                count=batch_size  # æ¯æ‰¹æ•°é‡
            )
            cursor = int(next_cursor)  # è½¬æ¢ä¸ºæ•´æ•°
            logger.info(f"â˜… æ‰«æåˆ° {len(keys)} ä¸ªé”® | æ–°æ¸¸æ ‡: {cursor}")  # æ–°å¢

            total_scanned += len(keys)
            if not keys and cursor == 0:  # é€€å‡ºæ¡ä»¶ï¼šæ— é”®ä¸”æ¸¸æ ‡å½’é›¶
                break

            # ====== 2.2 æ‰¹é‡è·å–è¿‡æœŸæ—¶é—´ ======
            # ä½¿ç”¨pipelineå‡å°‘ç½‘ç»œå¼€é”€
            with conn.pipeline() as pipe:
                for key in keys:
                    pipe.hget(key, 'expire_at')  # è·å–è¿‡æœŸæ—¶é—´å­—æ®µ
                expire_timestamps = pipe.execute()

            # ====== 2.3 è¿‡æ»¤è¿‡æœŸé”® ======
            current_time = time.time()
            expired_keys = []
            for key, expire_at in zip(keys, expire_timestamps):
                if not expire_at:
                    continue
                try:
                    if float(expire_at) < current_time:
                        expired_keys.append(key)
                except (TypeError, ValueError):
                    logger.warning(f"â³ æ— æ•ˆè¿‡æœŸæ—¶é—´: key={key}, value={expire_at}")

            logger.info(f"â˜… å‘ç° {len(expired_keys)} ä¸ªè¿‡æœŸé”®")

            # ====== 2.4 æ‰¹é‡è·å–æ–‡ä»¶è·¯å¾„ ======
            if expired_keys:
                with conn.pipeline() as pipe:
                    for key in expired_keys:
                        pipe.hget(key, 'real_path')
                    real_paths = pipe.execute()

                # è¿‡æ»¤æœ‰æ•ˆè·¯å¾„ æ„å»º (key, path)å¯¹
                valid_pairs = []
                for key, path in zip(expired_keys, real_paths):
                    if path and is_safe_path(path, log_errors=False):
                        valid_pairs.append((key, path))
                    else:
                        logger.warning(f"â³ å¿½ç•¥æ— æ•ˆè·¯å¾„: key={key}, path={path}")

                # æ·»åŠ åˆ°ç¼“å†²åŒºï¼ˆä¸å†ç«‹å³åˆ é™¤é”®ï¼‰
                expired_data_buffer.extend(valid_pairs)
                logger.info(f"â˜… æ–°å¢å¾…æ¸…ç†é¡¹: {len(valid_pairs)}")

                # ====== 2.5 è§¦å‘å¼‚æ­¥æ¸…ç†ï¼ˆå…³é”®ä¿®æ”¹ç‚¹ï¼‰=====
                if len(expired_data_buffer) >= 300:  # æ¯300æ¡æäº¤ä¸€æ¬¡
                    # æäº¤æ–‡ä»¶åˆ é™¤ä»»åŠ¡
                    file_paths = [path for _, path in expired_data_buffer]
                    safe_async_clean(file_paths)

                    # åˆ é™¤Redisé”®ï¼ˆä»…åœ¨ä»»åŠ¡æäº¤åæ‰§è¡Œï¼‰
                    keys_to_delete = [key for key, _ in expired_data_buffer]
                    for chunk in chunks(keys_to_delete, 100):
                        deleted = conn.delete(*chunk)
                        deleted_count += deleted
                        logger.info(f"ğŸ”‘ åˆ é™¤Redisé”®æ‰¹æ¬¡: æäº¤{len(chunk)} â†’ å®é™…åˆ é™¤{deleted}")

                    # æ¸…ç©ºç¼“å†²åŒº
                    expired_data_buffer.clear()

            # ====== 2.7 è¶…æ—¶ä¿æŠ¤ ======
            if time.time() - start_time > 250:  # é¢„ç•™50ç§’ç¼“å†²
                logger.warning("â³ æ¥è¿‘è¶…æ—¶ï¼Œç»ˆæ­¢æ‰«æ")
                break

            if cursor == 0:
                logger.info("â˜… SCANæ¸¸æ ‡å½’é›¶ï¼Œç»“æŸæ‰«æ")
                break  # å…³é”®ä¿®æ­£ï¼šæ— è®ºæ˜¯å¦æœ‰é”®éƒ½é€€å‡ºå¾ªç¯

        # ====== 3. æœ€ç»ˆæ¸…ç† ======
        if expired_data_buffer:
            # æäº¤å‰©ä½™æ–‡ä»¶åˆ é™¤ä»»åŠ¡
            file_paths = [path for _, path in expired_data_buffer]
            safe_async_clean(file_paths)

            # åˆ é™¤å‰©ä½™Redisé”®
            keys_to_delete = [key for key, _ in expired_data_buffer]
            deleted = conn.delete(*keys_to_delete)
            deleted_count += deleted
            logger.info(f"ğŸš® æœ€ç»ˆæäº¤: åˆ é™¤{deleted}ä¸ªé”®åŠæ–‡ä»¶")

        logger.info(f"â˜… ä¸»æ‰«æå¾ªç¯ç»“æŸ | æ€»æ‰«æ {total_scanned} é”® | å®é™…åˆ é™¤ {deleted_count} é”®")

        # æ–°å¢éƒ¨åˆ† - ä½¿ç”¨Redisè®°å½•æ‰«ææ¬¡æ•°
        scan_flag_key = "temp_cleanup:dir_scan_flag"
        try:
            scan_counter = int(conn.incr(scan_flag_key))
            logger.info(f"å½“å‰æ‰«æè®¡æ•°å™¨: {scan_counter}")
            conn.expire(scan_flag_key, 86400)  # 24å°æ—¶è¿‡æœŸ
            if scan_counter % 3 == 0:  # æ¯3æ¬¡æ‰§è¡Œè§¦å‘ä¸€æ¬¡
                scan_orphaned_files()
                conn.delete(scan_flag_key)  # é‡ç½®è®¡æ•°å™¨
        except Exception as e:
            logger.error(f"ç›®å½•æ‰«æè®¡æ•°å™¨å¼‚å¸¸: {str(e)}")

        return deleted_count

    except MemoryError:
        logger.critical("ğŸ›‘ å†…å­˜ä¿æŠ¤è§¦å‘ï¼Œç´§æ€¥é‡Šæ”¾èµ„æº")
        if expired_data_buffer:
            file_paths = [path for _, path in expired_data_buffer]
            safe_async_clean(file_paths)
        raise
    except Exception as e:
        logger.error(f"â€¼ï¸ æ¸…ç†å¼‚å¸¸: {str(e)}", exc_info=True)
        raise


def safe_async_clean(file_paths):
    """ç›´æ¥å¤„ç†æ–‡ä»¶è·¯å¾„çš„å¼‚æ­¥æ¸…ç†"""
    try:
        # åˆ†æ‰¹æ¬¡å¤„ç†é˜²æ­¢å†…å­˜æº¢å‡º
        for i in range(0, len(file_paths), 100):
            batch = file_paths[i:i + 100]

            # è¿‡æ»¤å®‰å…¨è·¯å¾„
            safe_paths = [p for p in batch if is_safe_path(p, log_errors=False)]
            if not safe_paths:
                continue

            # åˆ†å—æäº¤ä»»åŠ¡ï¼ˆæ¯å—10ä¸ªè·¯å¾„ï¼‰
            for path_chunk in chunks(safe_paths, 10):
                async_delete_file.chunks(
                    [(path,) for path in path_chunk], 10  # âœ… æ³¨æ„è¿™é‡Œä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼ç”Ÿæˆå‚æ•°å…ƒç»„
                ).apply_async(
                    queue='cleanup_low',
                    priority=9
                )

    except Exception as e:
        logger.error("æ–‡ä»¶æ¸…ç†ç®¡é“é”™è¯¯: %s", str(e))


@CeleryManager.get_celery().task(
    bind=True,
    autoretry_for=(Exception,),
    max_retries=2,
    retry_backoff=30,
    time_limit=120
)
def async_delete_file(self, path):
    """å¼‚æ­¥å®‰å…¨åˆ é™¤"""
    logger.info(f"ğŸ“¤ æ¥æ”¶åˆ°åˆ é™¤ä»»åŠ¡: {path}")
    try:

        if not is_safe_path(path):
            logger.error(f"éæ³•è·¯å¾„æ‹’ç»æ“ä½œ: {path}")
            return

        file_path = Path(path)
        if not file_path.exists():
            logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {path}")
            return

        # ç›®å½•æ¸…ç†ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        for attempt in range(3):
            try:
                file_path.unlink(missing_ok=True)
                logger.info(f"ğŸ—‘ï¸ æ–‡ä»¶åˆ é™¤æˆåŠŸ: {path}")
                # ç›®å½•æ¸…ç†ï¼ˆå¸¦çˆ¶çº§æ ¡éªŒï¼‰
                remove_empty_parents_safely(
                    file_path=file_path,
                    stop_at=Path(FileConfig.TEMP_DIR).resolve(),
                    max_retries=2
                )
                break
            except PermissionError:
                if attempt == 2:
                    logger.error(f"æƒé™ä¸è¶³: {path}")
                    raise
                time.sleep(1)
            except FileNotFoundError:
                break
            except Exception as e:
                logger.warning(f"åˆ é™¤å¼‚å¸¸[{attempt + 1}/3]: {path} - {str(e)}")
                time.sleep(0.5)

    except Exception as e:
        logger.error(f"ğŸ—‘ï¸ æ–‡ä»¶åˆ é™¤å¤±è´¥ï¼ˆ{self.retries}/2æ¬¡é‡è¯•ï¼‰: {path}", exc_info=True)
        raise self.retry(exc=e, countdown=30)
    finally:
        # å¼ºåˆ¶é‡Šæ”¾èµ„æº
        del file_path
        gc.collect()


def is_safe_path(path, log_errors=True):
    """è·¯å¾„å®‰å…¨æ£€æŸ¥"""
    try:
        resolved = Path(path).resolve()
        temp_base = Path(FileConfig.TEMP_DIR).resolve()
        safe = temp_base in resolved.parents
        if not safe and log_errors:
            logger.warning(f"è·¯å¾„ä¸å®‰å…¨: {path} -> {resolved}")
        return safe
    except Exception as e:
        if log_errors:
            logger.debug(f"è·¯å¾„è§£æå¼‚å¸¸: {path} - {str(e)}")
        return False


def chunks(items, chunk_size):
    """å¸¦ç©ºå€¼è¿‡æ»¤çš„åˆ†å—å‡½æ•°"""
    return (items[i:i + chunk_size] for i in range(0, len(items), chunk_size))


def safe_redis_decode(data):
    """å®‰å…¨è§£ç Redisè¿”å›å€¼"""
    if isinstance(data, bytes):
        return data.decode('utf-8', errors='replace')
    return str(data) if data is not None else ""


def scan_orphaned_files():
    """å…œåº•æ‰«ææš‚å­˜ç›®å½•ï¼ˆå®‰å…¨éå†ç‰ˆæœ¬ï¼‰"""
    logger.info("ğŸ å¼€å§‹æ‰§è¡Œå…œåº•ç›®å½•æ‰«æ...")
    temp_dir = Path(FileConfig.TEMP_DIR).resolve()
    count = 0
    total_files = 0

    # å®‰å…¨æ”¶é›†æ‰€æœ‰æ–‡ä»¶è·¯å¾„
    file_paths = []
    try:
        for entry in temp_dir.rglob('*'):
            try:
                if entry.is_file():
                    file_paths.append(entry)
            except FileNotFoundError:
                logger.debug("è·¯å¾„åœ¨æ‰«ææ—¶å·²è¢«åˆ é™¤: %s", entry)
    except FileNotFoundError as e:
        logger.warning("åŸºç¡€ç›®å½•ä¸å­˜åœ¨: %s", str(e))
        return 0

    # å¤„ç†æ”¶é›†åˆ°çš„æ–‡ä»¶
    for file_path in file_paths:
        try:
            # äºŒæ¬¡æ£€æŸ¥æ–‡ä»¶çŠ¶æ€ï¼ˆå¯èƒ½åœ¨æ”¶é›†åè¢«å…¶ä»–è¿›ç¨‹åˆ é™¤ï¼‰
            if not file_path.exists():
                logger.debug("æ–‡ä»¶å·²ä¸å­˜åœ¨ï¼Œè·³è¿‡: %s", file_path)
                continue

            total_files += 1
            logger.debug("æ‰«æåˆ°æ–‡ä»¶: %s", file_path)

            # å®‰å…¨æ£€æŸ¥è·¯å¾„æœ‰æ•ˆæ€§
            if not is_safe_path(str(file_path)):
                logger.warning("æ£€æµ‹åˆ°ä¸å®‰å…¨è·¯å¾„ï¼Œè·³è¿‡: %s", file_path)
                continue

            # æ£€æŸ¥ç›®å½•è¿‡æœŸçŠ¶æ€
            dir_expired = False
            current_dir = file_path.parent

            while current_dir != temp_dir:
                # å®‰å…¨è§£æè·¯å¾„é¿å…ç¬¦å·é“¾æ¥é—®é¢˜
                current_dir = current_dir.resolve()
                dir_name = current_dir.name

                # æ£€æµ‹æ—¶é—´æˆ³ç›®å½•æ ¼å¼
                if len(dir_name) == 14 and dir_name.isdigit():
                    try:
                        dir_time = datetime.strptime(dir_name, "%Y%m%d%H%M%S").timestamp()
                        if time.time() - dir_time > FileConfig.TEMP_FILE_MAX_AGE:
                            dir_expired = True
                            break
                    except ValueError:
                        pass  # éæ—¶é—´æˆ³ç›®å½•è·³è¿‡

                # å‘ä¸Šéå†æ—¶ç¡®ä¿è·¯å¾„æœ‰æ•ˆ
                if not current_dir.exists():
                    logger.debug("çˆ¶ç›®å½•å·²è¢«åˆ é™¤ï¼Œåœæ­¢å‘ä¸Šæ£€æŸ¥: %s", current_dir)
                    break
                current_dir = current_dir.parent

            if dir_expired:
                try:
                    TempFileService.cleanup_temp_files(file_path)
                    count += 1
                except Exception as e:
                    logger.error("ç›®å½•è¿‡æœŸæ–‡ä»¶åˆ é™¤å¤±è´¥ %s: %s", file_path, str(e))
                continue

            # æœ€åä¿®æ”¹æ—¶é—´æ£€æŸ¥
            try:
                file_mtime = file_path.stat().st_mtime
                if file_mtime < (time.time() - FileConfig.TEMP_FILE_MAX_AGE):
                    TempFileService.cleanup_temp_files(file_path)
                    count += 1
            except FileNotFoundError:
                logger.debug("æ–‡ä»¶å·²è¢«åˆ é™¤ï¼Œè·³è¿‡: %s", file_path)
            except Exception as e:
                logger.error("mtimeæ£€æŸ¥å¤±è´¥ %s: %s", file_path, str(e))

        except FileNotFoundError:
            logger.debug("æ–‡ä»¶å¤„ç†æ—¶å·²è¢«åˆ é™¤ï¼Œè·³è¿‡: %s", file_path)
        except Exception as e:
            logger.error("æ–‡ä»¶å¤„ç†å¼‚å¸¸ %s: %s", file_path, str(e), exc_info=True)

    logger.info("ğŸ“Š å…±æ‰«æåˆ°å­¤å„¿æ–‡ä»¶: %d | å·²æ¸…ç†æ•°é‡: %d", total_files, count)
    return count